# Vocabulary Flow Through the Pipeline

This diagram shows how vocabulary data is created, processed, and used in the chatbot system.

## 1. Vocabulary Creation Flow

```
Input Text Files (corpuses/*.txt)
         â†“
    [vocab_model.c]
         â†“
   Tokenization & Processing
         â†“
    vocab_model.txt
[Word Index] [Word] [Embedding] [PE] [Weight] [Bias1] [Bias2] [Bias3] [Bias4]
     1      start-token   0.84    0.00   0.39     0.00    0.00    0.00    0.00
     2      emoji         0.78    0.04   0.80     0.00    0.00    0.00    0.00
     3      test          0.91    0.08   0.19     0.00    0.00    0.00    0.00
     4      file          0.34    0.13   0.77     0.00    0.00    0.00    0.00
     5      ðŸ˜€            0.28    0.17   0.55     0.00    0.00    0.00    0.00
     6      ðŸ˜ƒ            0.48    0.21   0.63     0.00    0.00    0.00    0.00
     ...    ...           ...     ...    ...      ...     ...     ...     ...
```

## 2. Training Flow

```
vocab_model.txt â†’ [trainer.c] â†’ Updated Parameters
                      â†“
              Initializes Components
                      â†“
        [attention.c]  [mlp_layer.c]  [optimizer.c]
              â†“              â†“              â†“
    attention_model.txt  mlp_model.txt  optimizer_state.txt
              â†“              â†“              â†“
         Forward Pass Through Network ([forward_prop.c])
                      â†“
              Loss Computation & Backpropagation ([backward_prop.c])
                      â†“
              Parameter Updates (Adam Optimizer)
                      â†“
    All Model Files Updated (vocab_model.txt, attention_model.txt, etc.)
```

## 3. Prediction Flow

```
User Prompt â†’ [chatbot.c] â†’ Response
    â†“
Loads vocab_model.txt
    â†“
Tokenizes Input
    â†“
For Each Word:
  â”Œâ”€ Find word in vocabulary
  â”‚
  â”œâ”€ Get word vector: [embedding, pe, weight, bias1, bias2, bias3, bias4]
  â”‚
  â”œâ”€ Calculate dot product with all vocabulary words
  â”‚
  â”œâ”€ Apply softmax with temperature sampling
  â”‚
  â””â”€ Select next word from top candidates
    â†“
Continue until "end-token" or length limit
    â†“
Generated Response
```

## Key Data Transformations

### Initial Vocabulary Entry:
```
Word: "ðŸ˜€"
Embedding: 0.28  (Random initial value)
PE: 0.17         (Positional encoding)
Weight: 0.55     (Random initial value)
Bias1-4: 0.00    (Zero-initialized)
```

### After Training:
```
Word: "ðŸ˜€"
Embedding: 0.42  (Learned value)
PE: 0.17         (Unchanged)
Weight: 0.68     (Learned value)
Bias1: 0.05      (Learned value)
Bias2: -0.02     (Learned value)
Bias3: 0.01      (Learned value)
Bias4: 0.03      (Learned value)
```

### During Prediction:
```
Input: "emoji ðŸ˜€"
              â†“
Find "ðŸ˜€" in vocab_model.txt â†’ Index 5
              â†“
Get word vector: [0.42, 0.17, 0.68, 0.05, -0.02, 0.01, 0.03]
              â†“
Calculate similarity with all words in vocabulary
              â†“
Apply softmax â†’ Probability distribution
              â†“
Temperature sampling â†’ Select next word
              â†“
Generate response
```