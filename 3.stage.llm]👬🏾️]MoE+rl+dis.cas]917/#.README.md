./trainer.+x ../vocab_model.txt 
# 3-Stage LLM Pipeline

This project implements a simple 3-stage Large Language Model (LLM) pipeline in C. The pipeline consists of separate modules that communicate via a `vocab_model.txt` file and system calls.

## Modules

1.  **`vocab_model`**: Creates a vocabulary from text files.
2.  **`trainer`**: Simulates the training process by updating weights and biases in the vocabulary using a simplified MLP and attention mechanism.
3.  **`chatbot`**: Generates text based on a prompt and the trained vocabulary using temperature sampling.
4.  **`chatbot_moe_v1`**: Enhanced chatbot with Mixture of Experts (MOE) feature that can use multiple training sets simultaneously.
5.  **`meta_rl/meta_rl`**: Meta Reinforcement Learning orchestrator that intelligently selects curricula based on prompt analysis and learns from user feedback.

## Modularized Neural Network Components

The project has been refactored to include separate modules for each neural network component:

1.  **`forward_prop`**: Handles forward propagation through the attention mechanism and MLP layers.
2.  **`backward_prop`**: Handles backward propagation and gradient computation.
3.  **`mlp_layer`**: Implements the Multi-Layer Perceptron functionality.
4.  **`attention`**: Implements the self-attention mechanism.
5.  **`optimizer`**: Implements SGD with momentum for parameter updates.

## Tools

*   **`tools/cosine_similarity`**: Calculates the cosine similarity between two words in the vocabulary.

## How to Use

### Compilation

To compile all the modules and tools, run the compilation script:

```bash
cd +x
bash xsath.compile-all.+x]üîòÔ∏è.sh
```

This will create the following executables in the `+x` directory:

*   `vocab_model.+x`
*   `trainer.+x`
*   `chatbot.+x`
*   `chatbot_moe_v1` (new Mixture of Experts chatbot)
*   `forward_prop.+x`
*   `backward_prop.+x`
*   `mlp_layer.+x`
*   `attention.+x`
*   `optimizer.+x`
*   `tools/cosine_similarity`

You can also compile the MOE chatbot separately:
```bash
gcc -o chatbot_moe_v1 chatbot_moe_v1.c -lm
```

### 1. Create the Vocabulary

The `vocab_model` program takes one or more text files as input and creates a `vocab_model.txt` file. This file contains the vocabulary of the model, with each word having an associated embedding, weight, and biases.

```bash
./+x/vocab_model.+x corpuses/corpus_ying.txt corpuses/corpus_yang.txt
```

This will create a `vocab_model.txt` file in the root directory.

### 2. Train the Model

The `trainer` program takes the `vocab_model.txt` file as input and orchestrates the training process by calling the separate neural network component executables:

```bash
./+x/trainer.+x vocab_model.txt
```

To prevent the biases from being updated, use the `-bias` flag:

```bash
./+x/trainer.+x vocab_model.txt -bias
```

The trainer program will:
1. Initialize all neural network components (attention, MLP, optimizer)
2. Iterate through training epochs
3. For each word in the vocabulary:
   - Call `forward_prop.+x` to perform forward propagation
   - Compute loss (simulated in current implementation)
   - Update parameters using the optimizer
4. Save the updated `vocab_model.txt` file

### 3. Chat with the Bot

The `chatbot` program takes the `vocab_model.txt` file and a prompt as input and generates a response.

```bash
./+x/chatbot.+x vocab_model.txt "hello world"
```

You can also specify the desired length of the response and the sampling temperature (1-10):

```bash
./+x/chatbot.+x vocab_model.txt "hello world" 10 5
```

### 4. Chat with the MOE Bot (Mixture of Experts)

The `chatbot_moe_v1` program implements a Mixture of Experts feature that can use multiple training sets simultaneously. Instead of taking a single vocabulary file, it takes a `curriculum_bank.txt` file that lists multiple curriculum paths.

First, create a `curriculum_bank.txt` file with paths to multiple curriculum files:
```
/path/to/curriculum1/curriculum1.txt
/path/to/curriculum2/curriculum2.txt
```

Then run the MOE chatbot:
```bash
./chatbot_moe_v1 curriculum_bank.txt "hello world"
```

You can also specify the desired length of the response and the sampling temperature:
```bash
./chatbot_moe_v1 curriculum_bank.txt "hello world" 10 5
```

The MOE implementation preserves all entries from all vocabularies without deduplication, maintaining unique positional encodings that can be very helpful, especially in small curricula.

### 5. Meta RL Orchestrator

The `meta_rl/meta_rl` program implements a Meta Reinforcement Learning orchestrator that intelligently selects curricula based on prompt analysis and learns from user feedback. Instead of manually specifying which curricula to use, the meta RL orchestrator automatically determines the best combination.

The orchestrator works as follows:
1. Analyzes the prompt to extract features (word count, question marks, emojis, etc.)
2. Uses RL weights to score available curricula
3. Selects the most appropriate curricula for the prompt
4. Creates a temporary curriculum bank file
5. Runs the MOE chatbot with the selected curricula
6. Collects user feedback (1-10 scale)
7. Updates RL weights based on feedback using gradient ascent
8. Saves updated weights for future use

To use the meta RL orchestrator:
```bash
./meta_rl/meta_rl "hello world" 10 5
```

The orchestrator will:
1. Analyze the prompt "hello world"
2. Select appropriate curricula based on learned weights
3. Run the MOE chatbot with those curricula
4. Ask for your feedback on the response quality (1-10)
5. Update its RL weights based on your feedback

Over time, the meta RL orchestrator learns which curricula work best for different types of prompts, automatically improving its selection accuracy.

### 6. Using Modularized Components

Each neural network component can be used independently. A test script is provided to demonstrate usage:

```bash
./test_modules.sh
```

This script will:
1. Create a vocabulary model from corpus files
2. Initialize all neural network components
3. Run a forward propagation test

Or you can initialize each component manually:

Initialize attention module:
```bash
./+x/attention.+x init attention_model.txt
```

Initialize MLP layer:
```bash
./+x/mlp_layer.+x init mlp_model.txt
```

Initialize optimizer with learning rate 0.01 and momentum 0.9:
```bash
./+x/optimizer.+x init optimizer_state.txt 0.01 0.9
```

Run forward propagation:
```bash
./+x/forward_prop.+x vocab_model.txt 0
```

### 5. Calculate Cosine Similarity

The `cosine_similarity` tool calculates the cosine similarity between two words in the vocabulary.

```bash
./tools/cosine_similarity vocab_model.txt word1 word2
```

## Automatic Bias Updates

The biases in the `vocab_model.txt` file are automatically updated by the `trainer` module. This is by design and is intended to simulate a real-world training process where the biases are adjusted to improve the model's performance.

When the `vocab_model` program is first run, the biases are initialized to zero. The `trainer` program then adjusts these biases (along with the weights) to simulate the learning process. This is why the biases are not empty unless the `-bias` flag is used when running the `trainer`.

## Future Enhancements

### Meta Behavior Tree Framework

Planned future work includes developing a more robust meta behavior tree framework that will:

1. **Intelligently Trigger Different Components**: 
   - Automatically decide when to use RL-based curriculum selection vs. direct LLM prompting
   - Dynamically switch between different modes based on task complexity and context

2. **File/Directory Editing Capabilities**:
   - Integrate file system operations similar to Qwen-cli and other AI assistants
   - Enable the system to directly modify code, documents, and project structures
   - Implement safe file operations with automatic backup and rollback mechanisms

3. **Advanced Orchestration**:
   - Implement a behavior tree that can sequence complex workflows
   - Support conditional branching based on intermediate results
   - Enable parallel execution of independent tasks

4. **Enhanced Learning Capabilities**:
   - Extend beyond simple RL to include meta-learning approaches
   - Implement curriculum learning that adapts to user skill level
   - Add knowledge distillation capabilities for model compression

5. **Multi-Modal Integration**:
   - Extend support for image, audio, and video processing
   - Implement cross-modal reasoning capabilities
   - Add support for external APIs and services

This meta behavior tree will serve as a central orchestrator that can:
- Analyze user requests and determine the optimal response strategy
- Coordinate between different AI components (RL, LLM, rule-based systems)
- Manage file system operations and code modifications
- Track and learn from interaction outcomes to improve future performance

The framework will be designed to be modular and extensible, allowing for easy integration of new capabilities and components as they are developed.

## Conclusion

This project provides a comprehensive foundation for experimenting with neural language models, attention mechanisms, and reinforcement learning-based curriculum selection. The modular design enables easy experimentation with different configurations and components, while the meta RL orchestrator introduces intelligent automation for curriculum selection.
